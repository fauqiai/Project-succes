clustering_engine.py

import numpy as np

from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False


# =====================================
# MAIN CLUSTER WRAPPER
# =====================================

def cluster(
    X,
    method="hdbscan",   # "hdbscan", "gmm", "kmeans"
    k=8,
    random_state=42
):
    """
    Universal clustering wrapper.

    Returns:
        labels : np.array
        model  : fitted clustering object
    """

    if method == "hdbscan":

        if not HDBSCAN_AVAILABLE:
            print("‚ö†Ô∏è HDBSCAN not installed ‚Üí fallback to GMM")
            return _gmm_cluster(X, k, random_state)

        return _hdbscan_cluster(X)

    elif method == "gmm":
        return _gmm_cluster(X, k, random_state)

    elif method == "kmeans":
        return _kmeans_cluster(X, k, random_state)

    else:
        raise ValueError("Unknown clustering method")


# =====================================
# HDBSCAN (AUTO REGIME DETECTOR)
# =====================================

def _hdbscan_cluster(X):

    model = hdbscan.HDBSCAN(
        min_cluster_size=80,     # tweak later
        min_samples=20,
        metric='euclidean',
        cluster_selection_method='eom'
    )

    labels = model.fit_predict(X)

    return labels, model


# =====================================
# GAUSSIAN MIXTURE
# =====================================

def _gmm_cluster(X, k, random_state):

    model = GaussianMixture(
        n_components=k,
        covariance_type='full',
        random_state=random_state,
        n_init=5
    )

    labels = model.fit_predict(X)

    return labels, model


# =====================================
# KMEANS (SAFE FALLBACK)
# =====================================

def _kmeans_cluster(X, k, random_state):

    model = KMeans(
        n_clusters=k,
        n_init=25,
        random_state=random_state
    )

    labels = model.fit_predict(X)

    return labels, model


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("CLUSTERING ENGINE SELF TEST")

    import numpy as np

    # fake regimes
    np.random.seed(42)

    cluster1 = np.random.normal(0, 1, (500, 5))
    cluster2 = np.random.normal(5, 1, (500, 5))
    cluster3 = np.random.normal(-4, 1, (500, 5))

    X = np.vstack([cluster1, cluster2, cluster3])

    labels, model = cluster(X, method="hdbscan")

    unique = np.unique(labels)

    print("Clusters detected:", unique)
    print("Cluster count:", len(unique))

    noise = np.sum(labels == -1)
    print("Noise points:", noise)

    print("‚úÖ PASSED")









dimensionality_engine.py

import numpy as np
from sklearn.decomposition import PCA


# =====================================
# DIMENSION REDUCTION ENGINE
# =====================================

def reduce_dim(
    X,
    variance_threshold=0.90,   # keep 90% info
    max_components=20,
    model=None
):
    """
    Smart PCA reducer.

    Parameters:
    ----------------
    X : np.array
    variance_threshold : float
        target explained variance
    max_components : int
        hard cap to avoid over-reduction
    model : PCA object (optional)
        reuse for live trading later

    Returns:
    ----------------
    X_reduced
    model
    """

    # reuse existing PCA (IMPORTANT for live later)
    if model is not None:
        return model.transform(X), model

    # fit PCA
    pca_full = PCA()
    pca_full.fit(X)

    cumulative = np.cumsum(pca_full.explained_variance_ratio_)

    # auto choose components
    n_components = np.searchsorted(cumulative, variance_threshold) + 1

    # safety cap
    n_components = min(n_components, max_components)

    pca = PCA(n_components=n_components)

    X_reduced = pca.fit_transform(X)

    print(f"[Dimensionality] Reduced {X.shape[1]} ‚Üí {n_components} dimensions")
    print(f"[Dimensionality] Variance kept: {cumulative[n_components-1]:.2%}")

    return X_reduced, pca


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("DIMENSIONALITY ENGINE TEST")

    np.random.seed(42)

    # fake high-dim data
    X = np.random.normal(0, 1, (1500, 25))

    X_reduced, model = reduce_dim(X)

    print("Original shape:", X.shape)
    print("Reduced shape:", X_reduced.shape)

    print("PASSED ‚úÖ")







edge_validator.py

import pandas as pd
import numpy as np


# =====================================
# WALK FORWARD EDGE VALIDATION
# =====================================

def walkforward_edge_validation(
    df,
    state,
    forward=20,
    windows=6
):
    """
    Splits data into time windows
    and checks if edge survives.
    """

    future = df.close.shift(-forward)
    ret = (future - df.close) / df.close

    temp = pd.concat([
        state["cluster"],
        ret.rename("return")
    ], axis=1).dropna()

    size = len(temp)
    step = size // windows

    reports = []

    for i in range(windows):

        chunk = temp.iloc[i*step:(i+1)*step]

        stats = chunk.groupby("cluster")["return"].mean()

        reports.append(stats)

    result = pd.concat(reports, axis=1)
    result.columns = [f"window_{i}" for i in range(windows)]

    # stability score
    result["stability"] = result.std(axis=1)

    # mean edge
    result["avg_edge"] = result.mean(axis=1)

    # tradable filter
    result["tradable"] = (
        (result["avg_edge"].abs() > 0.001)
        & (result["stability"] < result["avg_edge"].abs())
    )

    return result.sort_values(
        "avg_edge",
        ascending=False
    )


# =====================================
# GLOBAL EDGE SCORE
# =====================================

def edge_health(result):

    tradable_ratio = result["tradable"].mean()

    if tradable_ratio > 0.5:
        return "REAL EDGE üî•"
    elif tradable_ratio > 0.3:
        return "POSSIBLE EDGE"
    elif tradable_ratio > 0.15:
        return "WEAK EDGE"
    else:
        return "ILLUSION ‚ùå"


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("EDGE VALIDATOR TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 3000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, *_ = build_state_matrix(f, r)

    state, _ = cluster_states(
        state,
        scaled,
        method="hdbscan"
    )

    report = walkforward_edge_validation(
        df,
        state
    )

    print(report)

    print("\nEDGE HEALTH:", edge_health(report))

    print("PASSED ‚úÖ")








expectancy_engine.py

import pandas as pd


def forward_returns(df, n=20):

    future = df.close.shift(-n)

    return (future - df.close) / df.close


def state_edge(df, state):

    fwd = forward_returns(df)

    table = (
        pd.concat([state.cluster, fwd], axis=1)
        .dropna()
        .groupby("cluster")[fwd.name]
        .agg(["mean","std","count"])
    )

    table["edge"] = table["mean"] / table["std"]

    return table.sort_values("edge", ascending=False)



if __name__ == "__main__":

    print("EXPECTANCY ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 2000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, _ = build_state_matrix(f,r)
    state, _ = cluster_states(state, scaled)

    print(state_edge(df, state).head())

    print("PASSED ‚úÖ")