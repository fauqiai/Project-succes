clustering_engine.py

import numpy as np

from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False


# =====================================
# MAIN CLUSTER WRAPPER
# =====================================

def cluster(
    X,
    method="hdbscan",
    k=8,
    random_state=42
):

    if method == "hdbscan":

        if not HDBSCAN_AVAILABLE:
            print("‚ö†Ô∏è HDBSCAN not installed ‚Üí fallback to GMM")
            return _gmm_cluster(X, k, random_state)

        labels, model = _hdbscan_cluster(X)

        unique = np.unique(labels)

        # üî• AUTO FAILSAFE
        if len(unique) <= 1:
            print("‚ö†Ô∏è HDBSCAN found no clusters ‚Üí auto fallback to GMM")
            return _gmm_cluster(X, k, random_state)

        return labels, model

    elif method == "gmm":
        return _gmm_cluster(X, k, random_state)

    elif method == "kmeans":
        return _kmeans_cluster(X, k, random_state)

    else:
        raise ValueError("Unknown clustering method")


# =====================================
# HDBSCAN (VERY LENIENT FOR M1 DATA)
# =====================================

def _hdbscan_cluster(X):

    import numpy as np
    import hdbscan

    n = len(X)

    # üî• lebih longgar lagi (‚âà0.15% data)
    min_cluster_size = max(10, int(n * 0.0015))   # ~40 utk 27k data
    min_samples = max(3, int(min_cluster_size * 0.15))

    print(f"HDBSCAN params ‚Üí min_cluster_size={min_cluster_size}, min_samples={min_samples}")

    model = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True
    )

    labels = model.fit_predict(X)

    print("Clusters found:", np.unique(labels))

    return labels, model


# =====================================
# GMM AUTO-SELECT (MAX 6 CLUSTERS)
# =====================================

def _gmm_cluster(X, k, random_state):

    import numpy as np
    from sklearn.mixture import GaussianMixture

    best_k = None
    best_bic = np.inf
    best_model = None

    # üî• hanya test 2 sampai 6 cluster (lebih realistis untuk market regime)
    for n_components in range(2, 7):

        model = GaussianMixture(
            n_components=n_components,
            covariance_type='full',
            random_state=random_state,
            n_init=5
        )

        model.fit(X)
        bic = model.bic(X)

        print(f"GMM test k={n_components} ‚Üí BIC={bic:.2f}")

        if bic < best_bic:
            best_bic = bic
            best_k = n_components
            best_model = model

    print(f"\n‚úÖ Best GMM clusters: {best_k}")

    labels = best_model.predict(X)

    print("GMM clusters:", np.unique(labels))

    return labels, best_model


# =====================================
# KMEANS
# =====================================

def _kmeans_cluster(X, k, random_state):

    k = min(k, max(2, len(X)//500))

    model = KMeans(
        n_clusters=k,
        n_init=25,
        random_state=random_state
    )

    labels = model.fit_predict(X)

    print("KMeans clusters:", np.unique(labels))

    return labels, model










dimensionality_engine.py

import numpy as np
from sklearn.decomposition import PCA


# =====================================
# DIMENSION REDUCTION ENGINE
# =====================================

def reduce_dim(
    X,
    variance_threshold=0.90,   # keep 90% info
    max_components=20,
    model=None
):
    """
    Smart PCA reducer.

    Parameters:
    ----------------
    X : np.array
    variance_threshold : float
        target explained variance
    max_components : int
        hard cap to avoid over-reduction
    model : PCA object (optional)
        reuse for live trading later

    Returns:
    ----------------
    X_reduced
    model
    """

    # reuse existing PCA (IMPORTANT for live later)
    if model is not None:
        return model.transform(X), model

    # fit PCA
    pca_full = PCA()
    pca_full.fit(X)

    cumulative = np.cumsum(pca_full.explained_variance_ratio_)

    # auto choose components
    n_components = np.searchsorted(cumulative, variance_threshold) + 1

    # safety cap
    n_components = min(n_components, max_components)

    pca = PCA(n_components=n_components)

    X_reduced = pca.fit_transform(X)

    print(f"[Dimensionality] Reduced {X.shape[1]} ‚Üí {n_components} dimensions")
    print(f"[Dimensionality] Variance kept: {cumulative[n_components-1]:.2%}")

    return X_reduced, pca


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("DIMENSIONALITY ENGINE TEST")

    np.random.seed(42)

    # fake high-dim data
    X = np.random.normal(0, 1, (1500, 25))

    X_reduced, model = reduce_dim(X)

    print("Original shape:", X.shape)
    print("Reduced shape:", X_reduced.shape)

    print("PASSED ‚úÖ")







edge_validator.py

import pandas as pd
import numpy as np


# =====================================
# WALK FORWARD EDGE VALIDATION
# =====================================

def walkforward_edge_validation(
    df,
    state,
    forward=20,
    windows=6
):
    """
    Splits data into time windows
    and checks if edge survives.
    """

    future = df.close.shift(-forward)
    ret = (future - df.close) / df.close

    temp = pd.concat([
        state["cluster"],
        ret.rename("return")
    ], axis=1).dropna()

    size = len(temp)
    step = size // windows

    reports = []

    for i in range(windows):

        chunk = temp.iloc[i*step:(i+1)*step]

        stats = chunk.groupby("cluster")["return"].mean()

        reports.append(stats)

    result = pd.concat(reports, axis=1)
    result.columns = [f"window_{i}" for i in range(windows)]

    # stability score
    result["stability"] = result.std(axis=1)

    # mean edge
    result["avg_edge"] = result.mean(axis=1)

    # tradable filter
    result["tradable"] = (
        (result["avg_edge"].abs() > 0.001)
        & (result["stability"] < result["avg_edge"].abs())
    )

    return result.sort_values(
        "avg_edge",
        ascending=False
    )


# =====================================
# GLOBAL EDGE SCORE
# =====================================

def edge_health(result):

    tradable_ratio = result["tradable"].mean()

    if tradable_ratio > 0.5:
        return "REAL EDGE üî•"
    elif tradable_ratio > 0.3:
        return "POSSIBLE EDGE"
    elif tradable_ratio > 0.15:
        return "WEAK EDGE"
    else:
        return "ILLUSION ‚ùå"


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("EDGE VALIDATOR TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 3000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, *_ = build_state_matrix(f, r)

    state, _ = cluster_states(
        state,
        scaled,
        method="hdbscan"
    )

    report = walkforward_edge_validation(
        df,
        state
    )

    print(report)

    print("\nEDGE HEALTH:", edge_health(report))

    print("PASSED ‚úÖ")








expectancy_engine.py

import pandas as pd


def forward_returns(df, n=20):

    future = df.close.shift(-n)

    return (future - df.close) / df.close


def state_edge(df, state):

    fwd = forward_returns(df)

    table = (
        pd.concat([state.cluster, fwd], axis=1)
        .dropna()
        .groupby("cluster")[fwd.name]
        .agg(["mean","std","count"])
    )

    table["edge"] = table["mean"] / table["std"]

    return table.sort_values("edge", ascending=False)



if __name__ == "__main__":

    print("EXPECTANCY ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 2000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, _ = build_state_matrix(f,r)
    state, _ = cluster_states(state, scaled)

    print(state_edge(df, state).head())

    print("PASSED ‚úÖ")



feature_discovery.py

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA


# =====================================
# AUTO FEATURE DISCOVERY
# =====================================

def discover_features(
    features,
    variance_keep=0.80,
    model=None
):
    """
    Creates hidden features using PCA.

    Think of this as:
    extracting latent market forces.
    """

    X = features.values

    # reuse model (live future)
    if model is not None:
        comps = model.transform(X)
    else:
        pca_full = PCA()
        pca_full.fit(X)

        cumulative = np.cumsum(
            pca_full.explained_variance_ratio_
        )

        n = np.searchsorted(
            cumulative,
            variance_keep
        ) + 1

        model = PCA(n_components=n)
        comps = model.fit_transform(X)

        print(f"[Discovery] Created {n} hidden features")

    cols = [
        f"latent_{i}"
        for i in range(comps.shape[1])
    ]

    discovered = pd.DataFrame(
        comps,
        index=features.index,
        columns=cols
    )

    return discovered, model


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("FEATURE DISCOVERY TEST")

    import numpy as np
    import pandas as pd

    np.random.seed(42)

    fake = pd.DataFrame(
        np.random.normal(0,1,(1500,10)),
        columns=[f"f{i}" for i in range(10)]
    )

    d, model = discover_features(fake)

    print(d.head())

    print("Discovered shape:", d.shape)

    print("PASSED ‚úÖ")









feature_engine.py

import pandas as pd
import numpy as np
from .measures import *


def build_feature_matrix(df):

    f = pd.DataFrame(index=df.index)

    # =====================
    # ORIGINAL MICRO FEATURES (tetap dipakai)
    # =====================
    f["volatility"] = volatility_ratio(df)
    f["expansion"] = expansion_ratio(df)
    f["momentum"] = momentum(df)
    f["liquidity"] = liquidity_penetration(df)
    f["rejection"] = wick_pressure(df)

    # hindari division zero
    f["compression"] = 1 / f["expansion"].replace(0, 1)

    # =====================
    # üî• CONTEXT FEATURES (multi-candle regime)
    # =====================

    close = df["close"]

    # --- Trend strength (EMA distance normalized)
    ema20 = close.ewm(span=20, adjust=False).mean()
    ema50 = close.ewm(span=50, adjust=False).mean()
    f["trend_fast"] = (close - ema20) / ema20.replace(0, 1)
    f["trend_slow"] = (ema20 - ema50) / ema50.replace(0, 1)

    # --- Rolling volatility regime
    ret = close.pct_change().fillna(0)
    f["vol_regime_20"] = ret.rolling(20).std().fillna(0)
    f["vol_regime_50"] = ret.rolling(50).std().fillna(0)

    # --- Range position in recent window (structure awareness)
    high_roll = df["high"].rolling(20).max()
    low_roll = df["low"].rolling(20).min()
    rng = (high_roll - low_roll).replace(0, 1)
    f["range_pos_20"] = (close - low_roll) / rng

    # --- Candle size relative to ATR-like range
    atr_like = (df["high"] - df["low"]).rolling(20).mean().replace(0, 1)
    f["candle_size_rel"] = (df["high"] - df["low"]) / atr_like

    # --- Momentum persistence (short vs medium return)
    f["ret_5"] = close.pct_change(5)
    f["ret_15"] = close.pct_change(15)

    # =====================
    # CLEANUP
    # =====================
    f = f.replace([np.inf, -np.inf], 0)
    return f.fillna(0)


if __name__ == "__main__":

    print("FEATURE ENGINE TEST")

    size = 500
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    print(build_feature_matrix(df).head())
    print("PASSED ‚úÖ")










interaction_engine.py

import pandas as pd
import numpy as np


# =====================================
# NORMALIZATION HELPER
# =====================================

def zscore(series, window=100):

    mean = series.rolling(window).mean()
    std = series.rolling(window).std()

    return (series - mean) / std.replace(0, np.nan)



# =====================================
# BUILD INTERACTIONS (MONSTER)
# =====================================

def build_interactions(features):

    inter = pd.DataFrame(index=features.index)

    vol = features["volatility"]
    comp = features["compression"]
    liq = features["liquidity"]
    mom = features["momentum"]
    rej = features["rejection"]

    # =====================================
    # EXPLOSION DETECTOR
    # compression + rising volatility
    # =====================================

    inter["vol_compression_break"] = zscore(vol * comp)

    # =====================================
    # LIQUIDITY EXPANSION
    # sweep + movement
    # =====================================

    inter["liquidity_expansion"] = zscore(liq * vol)

    # =====================================
    # TREND IGNITION
    # momentum confirmed by volatility
    # =====================================

    inter["trend_ignition"] = zscore(mom * vol)

    # =====================================
    # POSSIBLE EXHAUSTION
    # rejection inside high vol
    # =====================================

    inter["exhaustion_risk"] = zscore(rej * vol)

    # =====================================
    # TRAP DETECTOR
    # liquidity spike but weak momentum
    # =====================================

    trap_raw = liq * (1 / (abs(mom) + 0.0001))
    inter["trap_risk"] = zscore(trap_raw)

    # =====================================
    # CHAOS INDEX
    # volatility + rejection
    # =====================================

    chaos = vol * rej
    inter["chaos_index"] = zscore(chaos)

    # =====================================
    # TREND QUALITY
    # momentum without chaos
    # =====================================

    trend_quality = mom / (rej + 0.0001)
    inter["trend_quality"] = zscore(trend_quality)

    # =====================================
    # COMPRESSION PRESSURE
    # market storing energy
    # =====================================

    pressure = comp * (1 / (vol + 0.0001))
    inter["compression_pressure"] = zscore(pressure)

    return inter.replace([np.inf, -np.inf], np.nan).fillna(0)



# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("MONSTER INTERACTION ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix

    size = 1500
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)

    inter = build_interactions(f)

    print(inter.head())

    print("\nFeatures created:", len(inter.columns))

    print("PASSED ‚úÖ")










measures.py


import numpy as np
import pandas as pd


def atr(df, period=14):

    tr = pd.concat([
        df.high - df.low,
        (df.high - df.close.shift()).abs(),
        (df.low - df.close.shift()).abs()
    ], axis=1).max(axis=1)

    return tr.rolling(period, min_periods=1).mean()


def volatility_ratio(df, lookback=100):

    a = atr(df)
    base = a.rolling(lookback, min_periods=20).mean()

    return (a / base).fillna(1)


def expansion_ratio(df):

    return (df.high - df.low) / atr(df)


def momentum(df, n=5):

    return (df.close - df.close.shift(n)) / atr(df)


def liquidity_penetration(df, window=20):

    prev_high = df.high.rolling(window).max().shift(1)
    prev_low = df.low.rolling(window).min().shift(1)

    up = (df.high - prev_high).clip(lower=0)
    down = (prev_low - df.low).clip(lower=0)

    return ((up + down) / atr(df)).fillna(0)


def wick_pressure(df):

    rng = df.high - df.low
    upper = df.high - df[['open','close']].max(axis=1)
    lower = df[['open','close']].min(axis=1) - df.low

    return ((upper + lower) / rng.replace(0, np.nan)).fillna(0)



if __name__ == "__main__":

    print("MEASURES SELF TEST")

    import numpy as np

    size = 1000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    print(volatility_ratio(df).tail())
    print("PASSED ‚úÖ")







meta_monitor.py


import pandas as pd


def monitor_edge_decay(
    returns,
    window_short=50,
    window_long=300
):
    """
    Detect if edge is fading.
    """

    short = returns.rolling(window_short).mean()
    long = returns.rolling(window_long).mean()

    decay = short < long

    return decay.fillna(False)


def model_health(decay_series):

    recent = decay_series.tail(100).mean()

    if recent > 0.6:
        return "EDGE DYING"
    elif recent > 0.4:
        return "WEAKENING"
    else:
        return "HEALTHY"


# ============== SELF TEST ==============

if __name__ == "__main__":

    print("META MONITOR TEST")

    import numpy as np

    good = np.random.normal(0.003,0.01,800)
    bad = np.random.normal(-0.002,0.02,200)

    series = pd.Series(
        list(good) + list(bad)
    )

    decay = monitor_edge_decay(series)

    print("Decay signals:", decay.sum())
    print("Health:", model_health(decay))

    print("PASSED ‚úÖ")







regime_engine.py

import pandas as pd
from .measures import atr


def trend_strength(df):

    fast = df.close.rolling(20).mean()
    slow = df.close.rolling(50).mean()

    return ((fast - slow).abs() / atr(df)).fillna(0)


def volatility_regime(df):

    a = atr(df)
    base = a.rolling(200, min_periods=50).mean()

    return (a / base).fillna(1)


def volatility_slope(df):

    vol = atr(df)

    return vol.diff(10) / vol


def regime_shift(df):

    slope = volatility_slope(df)

    # magnitude of change
    return slope.abs()



def build_regime_matrix(df):

    r = pd.DataFrame(index=df.index)

    r["trend_strength"] = trend_strength(df)
    r["vol_regime"] = volatility_regime(df)
    r["vol_slope"] = volatility_slope(df)
    r["regime_shift"] = regime_shift(df)

    return r.fillna(0)



if __name__ == "__main__":

    print("REGIME ENGINE TEST")

    import numpy as np

    size = 800
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    print(build_regime_matrix(df).tail())

    print("PASSED ‚úÖ")








regime_shift_engine.py

import numpy as np
import pandas as pd


def detect_regime_shift(
    returns,
    window=200,
    z_threshold=2.5
):
    """
    Detect volatility / return distribution shift.
    """

    rolling_mean = returns.rolling(window).mean()
    rolling_std = returns.rolling(window).std()

    zscore = (
        (returns - rolling_mean) /
        rolling_std.replace(0, np.nan)
    )

    shift = zscore.abs() > z_threshold

    return shift.fillna(False)


def regime_shift_alert(shift_series):
    """
    Simple health signal.
    """

    recent = shift_series.tail(50).mean()

    if recent > 0.3:
        return "HIGH SHIFT"
    elif recent > 0.15:
        return "MODERATE SHIFT"
    else:
        return "STABLE"


# ================= SELF TEST =================

if __name__ == "__main__":

    print("REGIME SHIFT ENGINE TEST")

    np.random.seed(42)

    normal = np.random.normal(0,1,1000)
    crash = np.random.normal(-3,2,200)

    series = pd.Series(
        np.concatenate([normal, crash])
    )

    shift = detect_regime_shift(series)

    print("Shift detected:", shift.sum())
    print("Health:", regime_shift_alert(shift))

    print("PASSED ‚úÖ")








regime_validator.py

import pandas as pd
import numpy as np


# =====================================
# REGIME VALIDATOR
# =====================================

def validate_regimes(
    df,
    state,
    forward=20,
    min_samples=150,
    separation_threshold=0.002
):
    """
    Validate if clustered regimes are meaningful.

    Checks:
    --------
    ‚úÖ sample size
    ‚úÖ return separation
    ‚úÖ edge stability proxy

    Returns:
    --------
    report : DataFrame
    """

    future = df.close.shift(-forward)
    returns = (future - df.close) / df.close

    temp = pd.concat([
        state["cluster"],
        returns.rename("fwd_return")
    ], axis=1).dropna()

    grouped = temp.groupby("cluster")["fwd_return"]

    report = grouped.agg([
        ("count", "count"),
        ("mean_return", "mean"),
        ("volatility", "std")
    ])

    report["edge"] = report["mean_return"] / report["volatility"]

    # sample quality
    report["enough_samples"] = report["count"] > min_samples

    # separation test
    global_mean = temp["fwd_return"].mean()
    report["separated"] = (
        (report["mean_return"] - global_mean).abs()
        > separation_threshold
    )

    # simple score
    report["tradable"] = (
        report["enough_samples"] &
        report["separated"]
    )

    return report.sort_values("edge", ascending=False)


# =====================================
# QUICK HEALTH CHECK
# =====================================

def regime_health_score(report):
    """
    Returns a simple health metric.
    """

    tradable_ratio = report["tradable"].mean()

    if tradable_ratio > 0.6:
        return "STRONG"
    elif tradable_ratio > 0.4:
        return "DECENT"
    elif tradable_ratio > 0.2:
        return "WEAK"
    else:
        return "TRASH üòÑ"


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("REGIME VALIDATOR TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 2000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, scaler = build_state_matrix(f, r)
    state, model = cluster_states(state, scaled)

    report = validate_regimes(df, state)

    print(report)
    print("\nHEALTH:", regime_health_score(report))

    print("PASSED ‚úÖ")








state_engine.py


import pandas as pd
from sklearn.preprocessing import StandardScaler

from .clustering_engine import cluster
from .dimensionality_engine import reduce_dim
from .feature_discovery import discover_features


# =====================================
# BUILD STATE MATRIX (FINAL FORM)
# =====================================

def build_state_matrix(
    features,
    regimes,
    scaler=None,
    dim_model=None,
    discovery_model=None
):
    """
    FINAL PIPELINE:

    handcrafted features
        + discovered features
        + regimes
        ‚Üì
    scale
        ‚Üì
    reduce dimension
        ‚Üì
    READY for clustering
    """

    # üî• SCARY PART ‚Äî auto feature discovery
    discovered, discovery_model = discover_features(
        features,
        model=discovery_model
    )

    state = pd.concat([
        features,
        discovered,
        regimes
    ], axis=1).dropna()

    # reuse scaler for live later
    if scaler is None:
        scaler = StandardScaler()
        scaled = scaler.fit_transform(state)
    else:
        scaled = scaler.transform(state)

    # dimensionality reduction
    scaled, dim_model = reduce_dim(
        scaled,
        model=dim_model
    )

    return state, scaled, scaler, dim_model, discovery_model


# =====================================
# CLUSTER STATES
# =====================================

def cluster_states(
    state,
    scaled,
    method="hdbscan",
    k=8
):

    labels, model = cluster(
        scaled,
        method=method,
        k=k
    )

    state["cluster"] = labels

    return state, model


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("FINAL STATE ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .regime_validator import validate_regimes, regime_health_score

    size = 2500
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, scaler, dim_model, discovery_model = build_state_matrix(f, r)

    state, model = cluster_states(
        state,
        scaled,
        method="hdbscan"
    )

    print(state.head())

    report = validate_regimes(df, state)

    print("\nREGIME REPORT:")
    print(report)

    print("\nHEALTH:", regime_health_score(report))

    print("PASSED ‚úÖ")







transition_engine.py


import pandas as pd


def transition_matrix(states):

    cur = states.cluster
    nxt = cur.shift(-1)

    return pd.crosstab(cur, nxt, normalize="index")


def transition_expectancy(df, states, forward=20):

    future = df.close.shift(-forward)
    ret = (future - df.close) / df.close

    temp = pd.concat([
        states.cluster,
        states.cluster.shift(-1).rename("next"),
        ret
    ], axis=1).dropna()

    result = temp.groupby(["cluster","next"])[ret.name].mean()

    return result.unstack().fillna(0)



if __name__ == "__main__":

    print("TRANSITION ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 1500
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, _ = build_state_matrix(f,r)
    state, _ = cluster_states(state, scaled)

    print(transition_expectancy(df, state).head())

    print("PASSED ‚úÖ")







uncertainty_engine.py

import numpy as np


def bootstrap_confidence(
    returns,
    n_bootstrap=500
):
    """
    Estimate confidence using bootstrap.
    """

    means = []

    returns = np.array(returns.dropna())

    for _ in range(n_bootstrap):
        sample = np.random.choice(
            returns,
            size=len(returns),
            replace=True
        )
        means.append(sample.mean())

    lower = np.percentile(means, 5)
    upper = np.percentile(means, 95)

    confidence = 1 - (upper - lower)

    return {
        "mean": np.mean(means),
        "confidence": confidence,
        "range": (lower, upper)
    }


# ============== SELF TEST ==============

if __name__ == "__main__":

    print("UNCERTAINTY ENGINE TEST")

    import pandas as pd

    fake_returns = pd.Series(
        np.random.normal(0.002, 0.01, 1000)
    )

    result = bootstrap_confidence(fake_returns)

    print(result)

    print("PASSED ‚úÖ")