clustering_engine.py

import numpy as np

from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False


# =====================================
# MAIN CLUSTER WRAPPER
# =====================================

def cluster(
    X,
    method="hdbscan",   # "hdbscan", "gmm", "kmeans"
    k=8,
    random_state=42
):
    """
    Universal clustering wrapper.

    Returns:
        labels : np.array
        model  : fitted clustering object
    """

    if method == "hdbscan":

        if not HDBSCAN_AVAILABLE:
            print("‚ö†Ô∏è HDBSCAN not installed ‚Üí fallback to GMM")
            return _gmm_cluster(X, k, random_state)

        return _hdbscan_cluster(X)

    elif method == "gmm":
        return _gmm_cluster(X, k, random_state)

    elif method == "kmeans":
        return _kmeans_cluster(X, k, random_state)

    else:
        raise ValueError("Unknown clustering method")


# =====================================
# HDBSCAN (AUTO REGIME DETECTOR)
# =====================================

def _hdbscan_cluster(X):

    model = hdbscan.HDBSCAN(
        min_cluster_size=80,     # tweak later
        min_samples=20,
        metric='euclidean',
        cluster_selection_method='eom'
    )

    labels = model.fit_predict(X)

    return labels, model


# =====================================
# GAUSSIAN MIXTURE
# =====================================

def _gmm_cluster(X, k, random_state):

    model = GaussianMixture(
        n_components=k,
        covariance_type='full',
        random_state=random_state,
        n_init=5
    )

    labels = model.fit_predict(X)

    return labels, model


# =====================================
# KMEANS (SAFE FALLBACK)
# =====================================

def _kmeans_cluster(X, k, random_state):

    model = KMeans(
        n_clusters=k,
        n_init=25,
        random_state=random_state
    )

    labels = model.fit_predict(X)

    return labels, model


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("CLUSTERING ENGINE SELF TEST")

    import numpy as np

    # fake regimes
    np.random.seed(42)

    cluster1 = np.random.normal(0, 1, (500, 5))
    cluster2 = np.random.normal(5, 1, (500, 5))
    cluster3 = np.random.normal(-4, 1, (500, 5))

    X = np.vstack([cluster1, cluster2, cluster3])

    labels, model = cluster(X, method="hdbscan")

    unique = np.unique(labels)

    print("Clusters detected:", unique)
    print("Cluster count:", len(unique))

    noise = np.sum(labels == -1)
    print("Noise points:", noise)

    print("‚úÖ PASSED")









dimensionality_engine.py

import numpy as np
from sklearn.decomposition import PCA


# =====================================
# DIMENSION REDUCTION ENGINE
# =====================================

def reduce_dim(
    X,
    variance_threshold=0.90,   # keep 90% info
    max_components=20,
    model=None
):
    """
    Smart PCA reducer.

    Parameters:
    ----------------
    X : np.array
    variance_threshold : float
        target explained variance
    max_components : int
        hard cap to avoid over-reduction
    model : PCA object (optional)
        reuse for live trading later

    Returns:
    ----------------
    X_reduced
    model
    """

    # reuse existing PCA (IMPORTANT for live later)
    if model is not None:
        return model.transform(X), model

    # fit PCA
    pca_full = PCA()
    pca_full.fit(X)

    cumulative = np.cumsum(pca_full.explained_variance_ratio_)

    # auto choose components
    n_components = np.searchsorted(cumulative, variance_threshold) + 1

    # safety cap
    n_components = min(n_components, max_components)

    pca = PCA(n_components=n_components)

    X_reduced = pca.fit_transform(X)

    print(f"[Dimensionality] Reduced {X.shape[1]} ‚Üí {n_components} dimensions")
    print(f"[Dimensionality] Variance kept: {cumulative[n_components-1]:.2%}")

    return X_reduced, pca


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("DIMENSIONALITY ENGINE TEST")

    np.random.seed(42)

    # fake high-dim data
    X = np.random.normal(0, 1, (1500, 25))

    X_reduced, model = reduce_dim(X)

    print("Original shape:", X.shape)
    print("Reduced shape:", X_reduced.shape)

    print("PASSED ‚úÖ")







edge_validator.py

import pandas as pd
import numpy as np


# =====================================
# WALK FORWARD EDGE VALIDATION
# =====================================

def walkforward_edge_validation(
    df,
    state,
    forward=20,
    windows=6
):
    """
    Splits data into time windows
    and checks if edge survives.
    """

    future = df.close.shift(-forward)
    ret = (future - df.close) / df.close

    temp = pd.concat([
        state["cluster"],
        ret.rename("return")
    ], axis=1).dropna()

    size = len(temp)
    step = size // windows

    reports = []

    for i in range(windows):

        chunk = temp.iloc[i*step:(i+1)*step]

        stats = chunk.groupby("cluster")["return"].mean()

        reports.append(stats)

    result = pd.concat(reports, axis=1)
    result.columns = [f"window_{i}" for i in range(windows)]

    # stability score
    result["stability"] = result.std(axis=1)

    # mean edge
    result["avg_edge"] = result.mean(axis=1)

    # tradable filter
    result["tradable"] = (
        (result["avg_edge"].abs() > 0.001)
        & (result["stability"] < result["avg_edge"].abs())
    )

    return result.sort_values(
        "avg_edge",
        ascending=False
    )


# =====================================
# GLOBAL EDGE SCORE
# =====================================

def edge_health(result):

    tradable_ratio = result["tradable"].mean()

    if tradable_ratio > 0.5:
        return "REAL EDGE üî•"
    elif tradable_ratio > 0.3:
        return "POSSIBLE EDGE"
    elif tradable_ratio > 0.15:
        return "WEAK EDGE"
    else:
        return "ILLUSION ‚ùå"


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("EDGE VALIDATOR TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 3000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, *_ = build_state_matrix(f, r)

    state, _ = cluster_states(
        state,
        scaled,
        method="hdbscan"
    )

    report = walkforward_edge_validation(
        df,
        state
    )

    print(report)

    print("\nEDGE HEALTH:", edge_health(report))

    print("PASSED ‚úÖ")








expectancy_engine.py

import pandas as pd


def forward_returns(df, n=20):

    future = df.close.shift(-n)

    return (future - df.close) / df.close


def state_edge(df, state):

    fwd = forward_returns(df)

    table = (
        pd.concat([state.cluster, fwd], axis=1)
        .dropna()
        .groupby("cluster")[fwd.name]
        .agg(["mean","std","count"])
    )

    table["edge"] = table["mean"] / table["std"]

    return table.sort_values("edge", ascending=False)



if __name__ == "__main__":

    print("EXPECTANCY ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix
    from .regime_engine import build_regime_matrix
    from .state_engine import build_state_matrix, cluster_states

    size = 2000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)
    r = build_regime_matrix(df)

    state, scaled, _ = build_state_matrix(f,r)
    state, _ = cluster_states(state, scaled)

    print(state_edge(df, state).head())

    print("PASSED ‚úÖ")



feature_discovery.py

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA


# =====================================
# AUTO FEATURE DISCOVERY
# =====================================

def discover_features(
    features,
    variance_keep=0.80,
    model=None
):
    """
    Creates hidden features using PCA.

    Think of this as:
    extracting latent market forces.
    """

    X = features.values

    # reuse model (live future)
    if model is not None:
        comps = model.transform(X)
    else:
        pca_full = PCA()
        pca_full.fit(X)

        cumulative = np.cumsum(
            pca_full.explained_variance_ratio_
        )

        n = np.searchsorted(
            cumulative,
            variance_keep
        ) + 1

        model = PCA(n_components=n)
        comps = model.fit_transform(X)

        print(f"[Discovery] Created {n} hidden features")

    cols = [
        f"latent_{i}"
        for i in range(comps.shape[1])
    ]

    discovered = pd.DataFrame(
        comps,
        index=features.index,
        columns=cols
    )

    return discovered, model


# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("FEATURE DISCOVERY TEST")

    import numpy as np
    import pandas as pd

    np.random.seed(42)

    fake = pd.DataFrame(
        np.random.normal(0,1,(1500,10)),
        columns=[f"f{i}" for i in range(10)]
    )

    d, model = discover_features(fake)

    print(d.head())

    print("Discovered shape:", d.shape)

    print("PASSED ‚úÖ")









feature_engine.py

import pandas as pd
from .measures import *


def build_feature_matrix(df):

    f = pd.DataFrame(index=df.index)

    f["volatility"] = volatility_ratio(df)
    f["expansion"] = expansion_ratio(df)
    f["momentum"] = momentum(df)
    f["liquidity"] = liquidity_penetration(df)
    f["rejection"] = wick_pressure(df)

    f["compression"] = 1 / f["expansion"].replace(0,1)

    return f.fillna(0)



if __name__ == "__main__":

    print("FEATURE ENGINE TEST")

    import numpy as np

    size = 500
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    print(build_feature_matrix(df).head())

    print("PASSED ‚úÖ")









interaction_engine.py

import pandas as pd
import numpy as np


# =====================================
# NORMALIZATION HELPER
# =====================================

def zscore(series, window=100):

    mean = series.rolling(window).mean()
    std = series.rolling(window).std()

    return (series - mean) / std.replace(0, np.nan)



# =====================================
# BUILD INTERACTIONS (MONSTER)
# =====================================

def build_interactions(features):

    inter = pd.DataFrame(index=features.index)

    vol = features["volatility"]
    comp = features["compression"]
    liq = features["liquidity"]
    mom = features["momentum"]
    rej = features["rejection"]

    # =====================================
    # EXPLOSION DETECTOR
    # compression + rising volatility
    # =====================================

    inter["vol_compression_break"] = zscore(vol * comp)

    # =====================================
    # LIQUIDITY EXPANSION
    # sweep + movement
    # =====================================

    inter["liquidity_expansion"] = zscore(liq * vol)

    # =====================================
    # TREND IGNITION
    # momentum confirmed by volatility
    # =====================================

    inter["trend_ignition"] = zscore(mom * vol)

    # =====================================
    # POSSIBLE EXHAUSTION
    # rejection inside high vol
    # =====================================

    inter["exhaustion_risk"] = zscore(rej * vol)

    # =====================================
    # TRAP DETECTOR
    # liquidity spike but weak momentum
    # =====================================

    trap_raw = liq * (1 / (abs(mom) + 0.0001))
    inter["trap_risk"] = zscore(trap_raw)

    # =====================================
    # CHAOS INDEX
    # volatility + rejection
    # =====================================

    chaos = vol * rej
    inter["chaos_index"] = zscore(chaos)

    # =====================================
    # TREND QUALITY
    # momentum without chaos
    # =====================================

    trend_quality = mom / (rej + 0.0001)
    inter["trend_quality"] = zscore(trend_quality)

    # =====================================
    # COMPRESSION PRESSURE
    # market storing energy
    # =====================================

    pressure = comp * (1 / (vol + 0.0001))
    inter["compression_pressure"] = zscore(pressure)

    return inter.replace([np.inf, -np.inf], np.nan).fillna(0)



# =====================================
# SELF TEST
# =====================================

if __name__ == "__main__":

    print("MONSTER INTERACTION ENGINE TEST")

    import numpy as np
    from .feature_engine import build_feature_matrix

    size = 1500
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    f = build_feature_matrix(df)

    inter = build_interactions(f)

    print(inter.head())

    print("\nFeatures created:", len(inter.columns))

    print("PASSED ‚úÖ")










measures.py


import numpy as np
import pandas as pd


def atr(df, period=14):

    tr = pd.concat([
        df.high - df.low,
        (df.high - df.close.shift()).abs(),
        (df.low - df.close.shift()).abs()
    ], axis=1).max(axis=1)

    return tr.rolling(period, min_periods=1).mean()


def volatility_ratio(df, lookback=100):

    a = atr(df)
    base = a.rolling(lookback, min_periods=20).mean()

    return (a / base).fillna(1)


def expansion_ratio(df):

    return (df.high - df.low) / atr(df)


def momentum(df, n=5):

    return (df.close - df.close.shift(n)) / atr(df)


def liquidity_penetration(df, window=20):

    prev_high = df.high.rolling(window).max().shift(1)
    prev_low = df.low.rolling(window).min().shift(1)

    up = (df.high - prev_high).clip(lower=0)
    down = (prev_low - df.low).clip(lower=0)

    return ((up + down) / atr(df)).fillna(0)


def wick_pressure(df):

    rng = df.high - df.low
    upper = df.high - df[['open','close']].max(axis=1)
    lower = df[['open','close']].min(axis=1) - df.low

    return ((upper + lower) / rng.replace(0, np.nan)).fillna(0)



if __name__ == "__main__":

    print("MEASURES SELF TEST")

    import numpy as np

    size = 1000
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    print(volatility_ratio(df).tail())
    print("PASSED ‚úÖ")







meta_monitor.py


import pandas as pd


def monitor_edge_decay(
    returns,
    window_short=50,
    window_long=300
):
    """
    Detect if edge is fading.
    """

    short = returns.rolling(window_short).mean()
    long = returns.rolling(window_long).mean()

    decay = short < long

    return decay.fillna(False)


def model_health(decay_series):

    recent = decay_series.tail(100).mean()

    if recent > 0.6:
        return "EDGE DYING"
    elif recent > 0.4:
        return "WEAKENING"
    else:
        return "HEALTHY"


# ============== SELF TEST ==============

if __name__ == "__main__":

    print("META MONITOR TEST")

    import numpy as np

    good = np.random.normal(0.003,0.01,800)
    bad = np.random.normal(-0.002,0.02,200)

    series = pd.Series(
        list(good) + list(bad)
    )

    decay = monitor_edge_decay(series)

    print("Decay signals:", decay.sum())
    print("Health:", model_health(decay))

    print("PASSED ‚úÖ")







regime_engine.py

import pandas as pd
from .measures import atr


def trend_strength(df):

    fast = df.close.rolling(20).mean()
    slow = df.close.rolling(50).mean()

    return ((fast - slow).abs() / atr(df)).fillna(0)


def volatility_regime(df):

    a = atr(df)
    base = a.rolling(200, min_periods=50).mean()

    return (a / base).fillna(1)


def volatility_slope(df):

    vol = atr(df)

    return vol.diff(10) / vol


def regime_shift(df):

    slope = volatility_slope(df)

    # magnitude of change
    return slope.abs()



def build_regime_matrix(df):

    r = pd.DataFrame(index=df.index)

    r["trend_strength"] = trend_strength(df)
    r["vol_regime"] = volatility_regime(df)
    r["vol_slope"] = volatility_slope(df)
    r["regime_shift"] = regime_shift(df)

    return r.fillna(0)



if __name__ == "__main__":

    print("REGIME ENGINE TEST")

    import numpy as np

    size = 800
    price = np.cumsum(np.random.randn(size)) + 100

    df = pd.DataFrame({
        "open": price,
        "high": price + np.random.rand(size),
        "low": price - np.random.rand(size),
        "close": price
    })

    print(build_regime_matrix(df).tail())

    print("PASSED ‚úÖ")